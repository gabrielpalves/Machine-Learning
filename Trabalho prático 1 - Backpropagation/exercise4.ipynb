{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"clKNbb2IdK3P"},"source":["# Implementação do algoritmo de Backpropagation"]},{"cell_type":"code","execution_count":206,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1852,"status":"ok","timestamp":1652278533936,"user":{"displayName":"Mauro Roisenberg","userId":"14628479211278551410"},"user_tz":180},"id":"ts3oyKS8dK3b","outputId":"6ce53aca-484f-4f42-9b9d-a9680fe396e2"},"outputs":[],"source":["import os\n","import numpy as np\n","from matplotlib import pyplot\n","from scipy import optimize\n","%matplotlib inline"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[],"source":["def Theta_transform(layer_size, nn_params):\n","    aux = [-1]\n","    aux2 = [0]\n","    Thetas = list()\n","    for i in range(len(layer_size)-1):\n","        aux.append((layer_size[i]+1) * (layer_size[i+1]) + sum(aux2))\n","        aux2.append((layer_size[i]+1) * (layer_size[i+1]))\n","        aux3 = 1 if i == 0 else 0\n","        \n","        Thetas.append(\n","            np.reshape(nn_params[aux[i]+aux3:aux[i+1]],\n","                                (layer_size[i+1], (layer_size[i] + 1)))\n","        )\n","\n","    # for i in range(len(Thetas)):\n","    #     print(Thetas[i].shape)\n","    \n","    return Thetas\n"]},{"cell_type":"code","execution_count":208,"metadata":{"id":"LQbKZpF8eqSu"},"outputs":[],"source":["def displayData(X, example_width=None, figsize=(10, 10)):\n","    \"\"\"\n","    Displays 2D data stored in X in a nice grid.\n","    \"\"\"\n","    # Compute rows, cols\n","    if X.ndim == 2:\n","        m, n = X.shape\n","    elif X.ndim == 1:\n","        n = X.size\n","        m = 1\n","        X = X[None]  # Promote to a 2 dimensional array\n","    else:\n","        raise IndexError('Input X should be 1 or 2 dimensional.')\n","\n","    example_width = example_width or int(np.round(np.sqrt(n)))\n","    example_height = n / example_width\n","\n","    # Compute number of items to display\n","    display_rows = int(np.floor(np.sqrt(m)))\n","    display_cols = int(np.ceil(m / display_rows))\n","\n","    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)\n","    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n","\n","    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n","\n","    for i, ax in enumerate(ax_array):\n","        # Display Image\n","        h = ax.imshow(X[i].reshape(example_width, example_width, order='F'),\n","                      cmap='Greys', extent=[0, 1, 0, 1])\n","        ax.axis('off')\n","\n","\n","def predict(Theta1, Theta2, X):\n","    \"\"\"\n","    Predict the label of an input given a trained neural network\n","    Outputs the predicted label of X given the trained weights of a neural\n","    network(Theta1, Theta2)\n","    \"\"\"\n","    # Useful values\n","    m = X.shape[0]\n","    num_labels = Theta2.shape[0]\n","\n","    # You need to return the following variables correctly\n","    p = np.zeros(m)\n","    h1 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))\n","    h2 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), h1], axis=1), Theta2.T))\n","    p = np.argmax(h2, axis=1)\n","    return p\n","\n","\n","def debugInitializeWeights(fan_out, fan_in):\n","    \"\"\"\n","    Initialize the weights of a layer with fan_in incoming connections and fan_out outgoings\n","    connections using a fixed strategy. This will help you later in debugging.\n","\n","    Note that W should be set a matrix of size (1+fan_in, fan_out) as the first row of W handles\n","    the \"bias\" terms.\n","\n","    Parameters\n","    ----------\n","    fan_out : int\n","        The number of outgoing connections.\n","\n","    fan_in : int\n","        The number of incoming connections.\n","\n","    Returns\n","    -------\n","    W : array_like (1+fan_in, fan_out)\n","        The initialized weights array given the dimensions.\n","    \"\"\"\n","    # Initialize W using \"sin\". This ensures that W is always of the same values and will be\n","    # useful for debugging\n","    W = np.sin(np.arange(1, 1 + (1+fan_in)*fan_out))/10.0\n","    W = W.reshape(fan_out, 1+fan_in, order='F')\n","    return W\n","\n","\n","def computeNumericalGradient(J, theta, e=1e-4):\n","    \"\"\"\n","    Computes the gradient using \"finite differences\" and gives us a numerical estimate of the\n","    gradient.\n","\n","    Parameters\n","    ----------\n","    J : func\n","        The cost function which will be used to estimate its numerical gradient.\n","\n","    theta : array_like\n","        The one dimensional unrolled network parameters. The numerical gradient is computed at\n","         those given parameters.\n","\n","    e : float (optional)\n","        The value to use for epsilon for computing the finite difference.\n","\n","    Notes\n","    -----\n","    The following code implements numerical gradient checking, and\n","    returns the numerical gradient. It sets `numgrad[i]` to (a numerical\n","    approximation of) the partial derivative of J with respect to the\n","    i-th input argument, evaluated at theta. (i.e., `numgrad[i]` should\n","    be the (approximately) the partial derivative of J with respect\n","    to theta[i].)\n","    \"\"\"\n","    numgrad = np.zeros(theta.shape)\n","    perturb = np.diag(e * np.ones(theta.shape))\n","    for i in range(theta.size):\n","        loss1, _ = J(theta - perturb[:, i])\n","        loss2, _ = J(theta + perturb[:, i])\n","        numgrad[i] = (loss2 - loss1)/(2*e)\n","    return numgrad\n","\n","\n","def checkNNGradients(nnCostFunction, lambda_=0):\n","    \"\"\"\n","    Creates a small neural network to check the backpropagation gradients. It will output the\n","    analytical gradients produced by your backprop code and the numerical gradients\n","    (computed using computeNumericalGradient). These two gradient computations should result in\n","    very similar values.\n","\n","    Parameters\n","    ----------\n","    nnCostFunction : func\n","        A reference to the cost function implemented by the student.\n","\n","    lambda_ : float (optional)\n","        The regularization parameter value.\n","    \"\"\"\n","    input_layer_size = 3\n","    hidden_layer_size = 5\n","    num_labels = 3\n","    m = 5\n","\n","    # We generate some 'random' test data\n","    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n","    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n","\n","    # Reusing debugInitializeWeights to generate X\n","    X = debugInitializeWeights(m, input_layer_size - 1)\n","    y = np.arange(1, 1+m) % num_labels\n","    # print(y)\n","    # Unroll parameters\n","    nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n","    print(nn_params.shape)\n","\n","    # short hand for cost function\n","    costFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size,\n","                                        num_labels, X, y, lambda_, layer_size=[3, 5, 3])\n","    cost, grad = costFunc(nn_params)\n","    numgrad = computeNumericalGradient(costFunc, nn_params)\n","\n","    # Visually examine the two gradient computations.The two columns you get should be very similar.\n","    print(numgrad.shape, grad.shape)\n","    print(np.stack([numgrad, grad], axis=1))\n","    print('The above two columns you get should be very similar.')\n","    print('(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n')\n","\n","    # Evaluate the norm of the difference between two the solutions. If you have a correct\n","    # implementation, and assuming you used e = 0.0001 in computeNumericalGradient, then diff\n","    # should be less than 1e-9.\n","    diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad)\n","\n","    print('If your backpropagation implementation is correct, then \\n'\n","          'the relative difference will be small (less than 1e-9). \\n'\n","          'Relative Difference: %g' % diff)\n"]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[],"source":["def sigmoid(z):\n","    \"\"\"Sigmoide\"\"\"\n","    def s(z):\n","        return 1.0 / (1.0 + np.exp(-z))\n","    return s(z), s(z) * (1 - s(z))\n","\n","def sigmoidGradient(z):\n","    \"\"\"Derivada da função sigmoide\"\"\"\n","    return sigmoid(z) * (1 - sigmoid(z))\n","\n","def tanh(z):\n","    \"\"\"Tangente hiperbólica\"\"\"\n","    def th(z):\n","        return (np.exp(2*z) - 1)/(np.exp(2*z) + 1)\n","    return th(z), 1 - np.power(th(z), 2)\n","\n","def tanh_gradient(z):\n","    \"\"\"Derivada da tangente hiperbólica\"\"\"\n","    return 1 - np.power(tanh(z), 2)"]},{"cell_type":"code","execution_count":210,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3016,"status":"ok","timestamp":1652281056969,"user":{"displayName":"Mauro Roisenberg","userId":"14628479211278551410"},"user_tz":180},"id":"ZtWIczqdmMy_","outputId":"5a135de3-2f2b-4c4e-b6d7-cf9b41a44168"},"outputs":[],"source":["import scipy.io\n","data = scipy.io.loadmat('ex4data1.mat')"]},{"cell_type":"code","execution_count":211,"metadata":{"id":"9QCYCGBIdK3f"},"outputs":[],"source":["# training data stored in arrays X, y\n","X, y = data['X'], data['y'].ravel()\n","\n","# set the zero digit to 0, rather than its mapped 10 in this dataset\n","y[y == 10] = 0\n","\n","# Number of training examples\n","m = y.size"]},{"cell_type":"code","execution_count":212,"metadata":{"id":"hyudqzBodK3j"},"outputs":[],"source":["# Setup the parameters you will use for this exercise\n","input_layer_size  = 400  # 20x20 Input Images of Digits\n","hidden_layer_size = 25   # 25 hidden units\n","num_labels = 10          # 10 labels, from 0 to 9\n","\n","# Load the weights into variables Theta1 and Theta2\n","weights = scipy.io.loadmat('ex4weights.mat')\n","\n","# Theta1 has size 25 x 401\n","# Theta2 has size 10 x 26\n","Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n","\n","# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n","# since the weight file ex3weights.mat was saved based on MATLAB indexing\n","Theta2 = np.roll(Theta2, 1, axis=0)\n","\n","# Unroll parameters \n","nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[],"source":["layer_size = [400, 25, 10]\n","activation_function = sigmoid"]},{"cell_type":"code","execution_count":214,"metadata":{"id":"n2ey0gL-dK3l"},"outputs":[],"source":["def nnCostFunction(nn_params,\n","                   input_layer_size,\n","                   hidden_layer_size,\n","                   num_labels,\n","                   X, y, lambda_=0.0, activation_function=sigmoid, layer_size=[400, 25, 10]):\n","    \"\"\"\n","    Implements the neural network cost function and gradient for a two layer neural \n","    network which performs classification. \n","    \n","    Parameters\n","    ----------\n","    nn_params : array_like\n","        The parameters for the neural network which are \"unrolled\" into \n","        a vector. This needs to be converted back into the weight matrices Theta1\n","        and Theta2.\n","    \n","    input_layer_size : int\n","        Number of features for the input layer. \n","    \n","    hidden_layer_size : int\n","        Number of hidden units in the second layer.\n","    \n","    num_labels : int\n","        Total number of labels, or equivalently number of units in output layer. \n","    \n","    X : array_like\n","        Input dataset. A matrix of shape (m x input_layer_size).\n","    \n","    y : array_like\n","        Dataset labels. A vector of shape (m,).\n","    \n","    lambda_ : float, optional\n","        Regularization parameter.\n"," \n","    Returns\n","    -------\n","    J : float\n","        The computed value for the cost function at the current weight values.\n","    \n","    grad : array_like\n","        An \"unrolled\" vector of the partial derivatives of the concatenation of\n","        neural network weights Theta1 and Theta2.\n","    \n","    Instructions\n","    ------------\n","    You should complete the code by working through the following parts.\n","    \n","    - Part 1: Feedforward the neural network and return the cost in the \n","              variable J. After implementing Part 1, you can verify that your\n","              cost function computation is correct by verifying the cost\n","              computed in the following cell.\n","    \n","    - Part 2: Implement the backpropagation algorithm to compute the gradients\n","              Theta1_grad and Theta2_grad. You should return the partial derivatives of\n","              the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n","              Theta2_grad, respectively. After implementing Part 2, you can check\n","              that your implementation is correct by running checkNNGradients provided\n","              in the utils.py module.\n","    \n","              Note: The vector y passed into the function is a vector of labels\n","                    containing values from 0..K-1. You need to map this vector into a \n","                    binary vector of 1's and 0's to be used with the neural network\n","                    cost function.\n","     \n","              Hint: We recommend implementing backpropagation using a for-loop\n","                    over the training examples if you are implementing it for the \n","                    first time.\n","    \n","    - Part 3: Implement regularization with the cost function and gradients.\n","    \n","              Hint: You can implement this around the code for\n","                    backpropagation. That is, you can compute the gradients for\n","                    the regularization separately and then add them to Theta1_grad\n","                    and Theta2_grad from Part 2.\n","    \n","    Note \n","    ----\n","    We have provided an implementation for the sigmoid function in the file \n","    `utils.py` accompanying this assignment.\n","    \"\"\"\n","    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n","    # for our 2 layer neural network    \n","    Thetas = Theta_transform(layer_size, nn_params)\n","    \n","    # Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n","    #                     (hidden_layer_size, (input_layer_size + 1)))\n","\n","    # Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n","    #                     (num_labels, (hidden_layer_size + 1)))\n","\n","    # Setup some useful variables\n","    m = y.size\n","         \n","    # You need to return the following variables correctly \n","    J = 0\n","    # Theta1_grad = np.zeros(Theta1.shape)\n","    # Theta2_grad = np.zeros(Theta2.shape)\n","    \n","    # ====================== YOUR CODE HERE ======================\n","    \n","    activations = list()\n","    nets = [X]\n","    for i in range(len(layer_size)):\n","        if i == 0:\n","            activations.append(\n","                np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n","            )\n","        else:\n","            a = activations[i-1]\n","            Theta = Thetas[i-1]\n","            z = a.dot(Theta.T)\n","            nets.append(z)\n","            a_i, _ = activation_function(z)\n","            if i == len(layer_size)-1:\n","                activations.append(\n","                    a_i\n","                )\n","            else:\n","                activations.append(\n","                    np.concatenate([np.ones((a_i.shape[0], 1)), a_i], axis=1)\n","                )\n","\n","    # a1 = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n","\n","    # z2 = a1.dot(Theta1.T)\n","    # a2 = sigmoid(z2)\n","\n","    # a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n","\n","    # z3 = a2.dot(Theta2.T)\n","    # a3 = sigmoid(z3)\n","\n","    # Cálculo do custo J\n","    y_matrix = np.eye(num_labels)[y]\n","\n","    h = activations[-1] # hipótese\n","    J = (-1 / m) * np.sum((np.log(h) * y_matrix) + np.log(1 - h) * (1 - y_matrix))\n","    \n","    # Cálculo dos gradientes\n","    delta_end = h - y_matrix\n","    deltas = [delta_end]\n","    fraction = 1/m\n","    J_grads = [fraction * delta_end.T.dot(activations[-2])]\n","    for i in range(len(layer_size)-2, 0, -1):\n","        _, derivada = activation_function(nets[i])\n","        Theta = Thetas[i]\n","        delta = deltas[0]\n","        deltas = [np.matmul(delta, Theta[:,1:]) * derivada] + deltas # list concatenation\n","        \n","        delta = deltas[0]\n","        a = activations[i-1]\n","        J_grads = [fraction * delta.T.dot(a)] + J_grads # list concatenation\n","    \n","    grad = np.array([])\n","    for i in range(len(layer_size)-1):\n","        grad = np.concatenate((grad, J_grads[i].ravel()), axis=0)\n","    \n","    # delta_3 = h - y_matrix\n","    # delta_2 = np.matmul(delta_3,Theta2[:,1:]) * sigmoidGradient(z2)\n","\n","    # Theta1_grad = (1 / m) * delta_2.T.dot(a1)\n","    # Theta2_grad = (1 / m) * delta_3.T.dot(a2)\n","\n","    # Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n","    # Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n","    \n","    # ================================================================\n","    # Unroll gradients\n","    # grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n","\n","    return J, grad"]},{"cell_type":"code","execution_count":215,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":437,"status":"ok","timestamp":1652281435023,"user":{"displayName":"Mauro Roisenberg","userId":"14628479211278551410"},"user_tz":180},"id":"eApYXn-WdK3r","outputId":"56d4ea8b-7c7d-4e54-dabb-baf337dc4104"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cost at parameters (loaded from ex4weights): 0.287629 \n","The cost should be about                   : 0.287629.\n"]}],"source":["lambda_ = 0\n","J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n","                   num_labels, X, y, lambda_)\n","print('Cost at parameters (loaded from ex4weights): %.6f ' % J)\n","print('The cost should be about                   : 0.287629.')"]},{"cell_type":"code","execution_count":216,"metadata":{"id":"DJ3OskUWdK3x"},"outputs":[],"source":["def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n","    \"\"\"\n","    Randomly initialize the weights of a layer in a neural network.\n","    \n","    Parameters\n","    ----------\n","    L_in : int\n","        Number of incomming connections.\n","    \n","    L_out : int\n","        Number of outgoing connections. \n","    \n","    epsilon_init : float, optional\n","        Range of values which the weight can take from a uniform \n","        distribution.\n","    \n","    Returns\n","    -------\n","    W : array_like\n","        The weight initialiatized to random values.  Note that W should\n","        be set to a matrix of size(L_out, 1 + L_in) as\n","        the first column of W handles the \"bias\" terms.\n","        \n","    Instructions\n","    ------------\n","    Initialize W randomly so that we break the symmetry while training\n","    the neural network. Note that the first column of W corresponds \n","    to the parameters for the bias unit.\n","    \"\"\"\n","\n","    # You need to return the following variables correctly\n","    W = np.zeros((L_out, 1 + L_in))\n","\n","    # ====================== YOUR CODE HERE ======================\n","\n","    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n","\n","    # ============================================================\n","    return W"]},{"cell_type":"code","execution_count":217,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(10285,)\n"]}],"source":["# Todos os thetas em um só vetor\n","initial_nn_params = np.array([])\n","for i in range(len(layer_size)-1):\n","    initial_nn_params = np.concatenate((initial_nn_params, randInitializeWeights(layer_size[i], layer_size[i+1]).ravel()), axis=0)\n","print(initial_nn_params.shape)"]},{"cell_type":"code","execution_count":218,"metadata":{"id":"C3IAPN1ldK3x"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing Neural Network Parameters ...\n","(10285,) 10285 (25, 401) (10, 26)\n"]}],"source":["print('Initializing Neural Network Parameters ...')\n","    \n","initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n","initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n","\n","# Unroll parameters\n","initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)\n","print(initial_nn_params.shape, 25*401+10*26, initial_Theta1.shape, initial_Theta2.shape)"]},{"cell_type":"code","execution_count":219,"metadata":{"id":"SMZ3ENe5dK3y","outputId":"7e6ca1fe-fe73-4d84-8b1c-9424331d3e5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(38,)\n","(38,) (38,)\n","[[-9.27825235e-03 -9.27825236e-03]\n"," [-3.04978709e-06 -3.04978914e-06]\n"," [-1.75060084e-04 -1.75060082e-04]\n"," [-9.62660640e-05 -9.62660620e-05]\n"," [ 8.89911959e-03  8.89911960e-03]\n"," [ 1.42869450e-05  1.42869443e-05]\n"," [ 2.33146358e-04  2.33146357e-04]\n"," [ 1.17982666e-04  1.17982666e-04]\n"," [-8.36010761e-03 -8.36010762e-03]\n"," [-2.59383093e-05 -2.59383100e-05]\n"," [-2.87468729e-04 -2.87468729e-04]\n"," [-1.37149709e-04 -1.37149706e-04]\n"," [ 7.62813550e-03  7.62813551e-03]\n"," [ 3.69883257e-05  3.69883234e-05]\n"," [ 3.35320351e-04  3.35320347e-04]\n"," [ 1.53247082e-04  1.53247082e-04]\n"," [-6.74798369e-03 -6.74798370e-03]\n"," [-4.68759742e-05 -4.68759769e-05]\n"," [-3.76215583e-04 -3.76215587e-04]\n"," [-1.66560294e-04 -1.66560294e-04]\n"," [ 3.14544970e-01  3.14544970e-01]\n"," [ 1.64090819e-01  1.64090819e-01]\n"," [ 1.64567932e-01  1.64567932e-01]\n"," [ 1.58339334e-01  1.58339334e-01]\n"," [ 1.51127527e-01  1.51127527e-01]\n"," [ 1.49568335e-01  1.49568335e-01]\n"," [ 1.11056588e-01  1.11056588e-01]\n"," [ 5.75736494e-02  5.75736493e-02]\n"," [ 5.77867378e-02  5.77867378e-02]\n"," [ 5.59235296e-02  5.59235296e-02]\n"," [ 5.36967009e-02  5.36967009e-02]\n"," [ 5.31542052e-02  5.31542052e-02]\n"," [ 9.74006970e-02  9.74006970e-02]\n"," [ 5.04575855e-02  5.04575855e-02]\n"," [ 5.07530173e-02  5.07530173e-02]\n"," [ 4.91620841e-02  4.91620841e-02]\n"," [ 4.71456249e-02  4.71456249e-02]\n"," [ 4.65597186e-02  4.65597186e-02]]\n","The above two columns you get should be very similar.\n","(Left-Your Numerical Gradient, Right-Analytical Gradient)\n","\n","If your backpropagation implementation is correct, then \n","the relative difference will be small (less than 1e-9). \n","Relative Difference: 2.34726e-11\n"]}],"source":["checkNNGradients(nnCostFunction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVgsZXLMdK31"},"outputs":[],"source":["#  Check gradients by running checkNNGradients\n","lambda_ = 3\n","checkNNGradients(nnCostFunction, lambda_)\n","\n","# Also output the costFunction debugging values\n","debug_J, _  = nnCostFunction(nn_params, input_layer_size,\n","                          hidden_layer_size, num_labels, X, y, lambda_)\n","\n","print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))\n","print('(for lambda = 3, this value should be about 0.576051)')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUfO-EyOdK32"},"outputs":[],"source":["#  After you have completed the assignment, change the maxiter to a larger\n","#  value to see how more training helps.\n","options= {'maxiter': 400}\n","\n","#  You should also try different values of lambda\n","lambda_ = 1\n","\n","# Create \"short hand\" for the cost function to be minimized\n","costFunction = lambda p: nnCostFunction(p, input_layer_size,\n","                                        hidden_layer_size,\n","                                        num_labels, X, y, lambda_)\n","\n","# Now, costFunction is a function that takes in only one argument\n","# (the neural network parameters)\n","res = optimize.minimize(costFunction,\n","                        initial_nn_params,\n","                        jac=True,\n","                        method='TNC',\n","                        options=options)\n","\n","# get the solution of the optimization\n","nn_params = res.x\n","        \n","# Obtain Theta1 and Theta2 back from nn_params\n","Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n","                    (hidden_layer_size, (input_layer_size + 1)))\n","\n","Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n","                    (num_labels, (hidden_layer_size + 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7M9q63GNdK32"},"outputs":[],"source":["pred = predict(Theta1, Theta2, X)\n","print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYZtAH9MdK33"},"outputs":[],"source":["displayData(Theta1[:, 1:])"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Cópia de exercise4.ipynb","provenance":[{"file_id":"1XKlDNxAucosL5wgQhJS5VAKBSb15v-Jc","timestamp":1652290783688}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
